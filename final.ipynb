{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link for Downloading \"best_model.h5\" and \"best_model_cl.json\". \n",
    "#https://drive.google.com/open?id=1-0L1P3O4s0Hv-lL-ZWXZUlbw6U7dTy7Q \n",
    "#https://drive.google.com/open?id=1-4UqEmlqCB4G0m-W1uvf4otDFGiUf0sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3268,
     "status": "ok",
     "timestamp": 1587818985249,
     "user": {
      "displayName": "Bunny Ryan",
      "photoUrl": "https://lh4.googleusercontent.com/-qiGfq9smB-Q/AAAAAAAAAAI/AAAAAAAAG8A/yJ_CVhzEkko/s64/photo.jpg",
      "userId": "15945980258528082766"
     },
     "user_tz": -330
    },
    "id": "T2LUtCKAXTTo",
    "outputId": "2a62130a-cd1b-49b4-fb69-3101a64ed49c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries.\n",
    "from datetime import datetime \n",
    "start_real = datetime.now()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate\n",
    "from keras.layers import GRU, Embedding, Flatten, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Dense, Dropout, Flatten, Conv1D, GlobalMaxPooling1D, BatchNormalization, LSTM, GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLHF7ZklXc_E"
   },
   "outputs": [],
   "source": [
    "# Defining Utility Functions :\n",
    "\n",
    "# 1. RMSLE Function \n",
    "def rmsle(y, y_pred): # return Rmsle value.\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y )))  \n",
    "    \n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# 2. Word Count Function.\n",
    "def word_count(text):\n",
    "    try:\n",
    "        if text == 'No description yet':\n",
    "            return 0 # for the data point with string \"No description yet\" returns word count 0.\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            words = []\n",
    "            for w in text.split(\" \"):\n",
    "              words.append(w)\n",
    "            return len(words)\n",
    "    except: \n",
    "        return 0\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 3.Splitting category into sub categories.\n",
    "def cat_split(column, sub_cat):\n",
    "    category = []\n",
    "    for i in range(len(train)):\n",
    "        try:\n",
    "            category.append(train[column].values[i].split(\"/\")[sub_cat])\n",
    "        except:\n",
    "            category.append(\"No Label\") # If there is no sub category it repalces No Label.\n",
    "            \n",
    "    return category\n",
    "\n",
    "def cat_split_test(column, sub_cat, test):\n",
    "    category = []\n",
    "    for i in range(len(test)):\n",
    "        try:\n",
    "            category.append(test[column].values[i].split(\"/\")[sub_cat])\n",
    "        except:\n",
    "            category.append(\"No Label\") # If there is no sub category it repalces No Label.\n",
    "    return category\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 4. finding missing brands.\n",
    "#https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\n",
    "# The Brand Name has 600,000 Missing Values. This Function will replace the data. \n",
    "def finding_brand(row_n):\n",
    "    brand_row = row_n[0]\n",
    "    name = row_n[1]\n",
    "    namesplit = name.split(' ') # for missing brand we check every word in the name column\n",
    "    if brand_row == 'missing':\n",
    "        for x in namesplit: # Then we check for every word in brand vocabulary.if exists retur name\n",
    "            if x in brand_vocab:\n",
    "                return name \n",
    "    if name in brand_vocab:\n",
    "        return name\n",
    "    return brand_row\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# 5. Filling Missing values. Filling Columns names, category_name, item_description, brand_name \n",
    "def fill_missing_values(df):\n",
    "    df.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.replace('No description yet',\"missing\", inplace=True)\n",
    "    return df\n",
    "\n",
    "# https://stackoverflow.com/a/56876351\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9sPw84jXi2G"
   },
   "outputs": [],
   "source": [
    "def final_fun_1(data_point):\n",
    "  test = pd.DataFrame(data_point).T\n",
    "  test.columns = ['id', 'name', 'item_condition_id', 'category_name', 'brand_name','price', 'shipping', 'item_description']\n",
    "  test['desc_len'] = test['item_description'].apply(lambda x: word_count(x))\n",
    "  test['name_len'] = test['name'].apply(lambda x: word_count(x))\n",
    "  test[\"subcat_0\"] = cat_split_test(\"category_name\",0, test)\n",
    "  test[\"subcat_1\"] = cat_split_test(\"category_name\",1, test)\n",
    "  test[\"subcat_2\"] = cat_split_test(\"category_name\",2, test)\n",
    "  global brand_vocab\n",
    "  brand_vocab = set(test['brand_name'].values) \n",
    "  test.brand_name.fillna(value = \"missing\", inplace = True)\n",
    "  missing = len(test.loc[test[\"brand_name\"] == 'missing'])\n",
    "  test['brand_name'] = test[['brand_name','name']].apply(finding_brand, axis = 1)\n",
    "  detected_brands = missing-len(test.loc[test['brand_name'] == 'missing'])\n",
    "  test = fill_missing_values(test)\n",
    "  test[\"combined_text\"] = test[\"item_description\"] + \" \" + test[\"name\"]\n",
    "  test['brand_name'] = test['brand_name'].fillna('missing').astype(str)\n",
    "  label = LabelEncoderExt()\n",
    "  label.fit(np.hstack([test.brand_name]))\n",
    "  test['brand_name'] = label.transform(test.brand_name)\n",
    "  del label \n",
    "  label = LabelEncoderExt()\n",
    "  label.fit(np.hstack([test.category_name])) # categories united \n",
    "  test['category'] = label.transform(test.category_name)\n",
    "  label.fit(np.hstack([test.subcat_0])) # sub_cat0\n",
    "  test.subcat_0 = label.transform(test.subcat_0)\n",
    "  label.fit(np.hstack([test.subcat_1])) # sub_cat_1\n",
    "  test.subcat_1 = label.transform(test.subcat_1)\n",
    "  label.fit(np.hstack([test.subcat_2])) # sub_cat2\n",
    "  test.subcat_2 = label.transform(test.subcat_2)\n",
    "  del label \n",
    "  full_text = np.hstack([test.item_description.str.lower(), test.name.str.lower(), test.category_name.str.lower()]) \n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(full_text)\n",
    "  test[\"seq_combined\"] = tokenizer.texts_to_sequences(test.combined_text.str.lower())\n",
    "  test['seq_desc'] = tokenizer.texts_to_sequences(test.item_description.str.lower()) \n",
    "  test['seq_name'] = tokenizer.texts_to_sequences(test.name.str.lower()) \n",
    "  max_len_combined = np.max([test.seq_combined.max()]) + 1\n",
    "  max_len_brand =  np.max([test.brand_name.max()]) + 1   # brand # brand \n",
    "  max_len_condition = np.max([test.item_condition_id]) + 1  # item_cond\n",
    "  max_len_desc = np.max([int(int(test.desc_len.max()))]) + 1  # item_desc_len\n",
    "  max_len_name = np.max([int(int(test.name_len.max()))]) + 1 # name_len\n",
    "  max_len_sub0 = np.max([int(int(test.subcat_0.max()))])  + 1# Sub_0\n",
    "  max_len_sub1 = np.max([int( int(test.subcat_1.max()))]) + 1 # Sub_1\n",
    "  max_len_sub2 = np.max([ test.subcat_2.max()]) + 1  # Sub_2\n",
    "  name_padding = 10\n",
    "  description_padding = 70\n",
    "  combined_padding  =  70\n",
    "  max_len = np.max([np.max(test.seq_name.max()),np.max(test.seq_desc.max())]) + 1\n",
    "  te_data = {\n",
    "  \"name\" : pad_sequences(test.seq_name, maxlen= name_padding),\n",
    "  \"item_desc\" : pad_sequences(test.seq_desc, maxlen= description_padding),\n",
    "  \"brand_name\" : np.array(test.brand_name),\n",
    "  \"category\" : np.array(test.category),\n",
    "  \"item_condition\" : np.array(test.item_condition_id),\n",
    "  \"shipping\" : np.array(test[[\"shipping\"]]),\n",
    "  \"desc_len\" : np.array(test[[\"desc_len\"]]),\n",
    "  \"name_len\" : np.array(test[[\"name_len\"]]),\n",
    "  \"subcat_0\" : np.array(test.subcat_0),\n",
    "  \"subcat_1\" : np.array(test.subcat_1),\n",
    "  \"subcat_2\" : np.array(test.subcat_2),\n",
    "  \"combined_text\" : pad_sequences(test.seq_combined, maxlen= combined_padding)\n",
    "  }\n",
    "  X_test = te_data\n",
    "  json_file = open('/content/drive/My Drive/best_model_cl.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  loaded_model = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  loaded_model.load_weights(\"/content/drive/My Drive/best_model.h5\")\n",
    "  y_pred = loaded_model.predict(X_test)\n",
    "  test_pred = np.expm1(y_pred)\n",
    "  return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTEXh2ggXkhl"
   },
   "outputs": [],
   "source": [
    "def final_fun_2(data_point, target):\n",
    "  test = pd.DataFrame(data_point).T\n",
    "  test.columns = ['id', 'name', 'item_condition_id', 'category_name', 'brand_name','price', 'shipping', 'item_description']\n",
    "  y = np.log1p(target)\n",
    "  test['desc_len'] = test['item_description'].apply(lambda x: word_count(x))\n",
    "  test['name_len'] = test['name'].apply(lambda x: word_count(x))\n",
    "  test[\"subcat_0\"] = cat_split_test(\"category_name\",0, test)\n",
    "  test[\"subcat_1\"] = cat_split_test(\"category_name\",1, test)\n",
    "  test[\"subcat_2\"] = cat_split_test(\"category_name\",2, test)\n",
    "  global brand_vocab\n",
    "  brand_vocab = set(test['brand_name'].values) \n",
    "  test.brand_name.fillna(value = \"missing\", inplace = True)\n",
    "  missing = len(test.loc[test[\"brand_name\"] == 'missing'])\n",
    "  test['brand_name'] = test[['brand_name','name']].apply(finding_brand, axis = 1)\n",
    "  detected_brands = missing-len(test.loc[test['brand_name'] == 'missing'])\n",
    "  test = fill_missing_values(test)\n",
    "  test[\"combined_text\"] = test[\"item_description\"] + \" \" + test[\"name\"]\n",
    "  test['brand_name'] = test['brand_name'].fillna('missing').astype(str)\n",
    "  label = LabelEncoderExt()\n",
    "  label.fit(np.hstack([test.brand_name]))\n",
    "  test['brand_name'] = label.transform(test.brand_name)\n",
    "  del label \n",
    "  label = LabelEncoderExt()\n",
    "  label.fit(np.hstack([test.category_name])) # categories united \n",
    "  test['category'] = label.transform(test.category_name)\n",
    "  label.fit(np.hstack([test.subcat_0])) # sub_cat0\n",
    "  test.subcat_0 = label.transform(test.subcat_0)\n",
    "  label.fit(np.hstack([test.subcat_1])) # sub_cat_1\n",
    "  test.subcat_1 = label.transform(test.subcat_1)\n",
    "  label.fit(np.hstack([test.subcat_2])) # sub_cat2\n",
    "  test.subcat_2 = label.transform(test.subcat_2)\n",
    "  del label \n",
    "  full_text = np.hstack([test.item_description.str.lower(), test.name.str.lower(), test.category_name.str.lower()]) \n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(full_text)\n",
    "  test[\"seq_combined\"] = tokenizer.texts_to_sequences(test.combined_text.str.lower())\n",
    "  test['seq_desc'] = tokenizer.texts_to_sequences(test.item_description.str.lower()) \n",
    "  test['seq_name'] = tokenizer.texts_to_sequences(test.name.str.lower()) \n",
    "  max_len_combined = np.max([test.seq_combined.max()]) + 1\n",
    "  max_len_brand =  np.max([test.brand_name.max()]) + 1   # brand # brand \n",
    "  max_len_condition = np.max([test.item_condition_id]) + 1  # item_cond\n",
    "  max_len_desc = np.max([int(int(test.desc_len.max()))]) + 1  # item_desc_len\n",
    "  max_len_name = np.max([int(int(test.name_len.max()))]) + 1 # name_len\n",
    "  max_len_sub0 = np.max([int(int(test.subcat_0.max()))])  + 1# Sub_0\n",
    "  max_len_sub1 = np.max([int( int(test.subcat_1.max()))]) + 1 # Sub_1\n",
    "  max_len_sub2 = np.max([ test.subcat_2.max()]) + 1  # Sub_2\n",
    "  name_padding = 10\n",
    "  description_padding = 70\n",
    "  combined_padding  =  70\n",
    "  max_len = np.max([np.max(test.seq_name.max()),np.max(test.seq_desc.max())]) + 1\n",
    "  te_data = {\n",
    "  \"name\" : pad_sequences(test.seq_name, maxlen= name_padding),\n",
    "  \"item_desc\" : pad_sequences(test.seq_desc, maxlen= description_padding),\n",
    "  \"brand_name\" : np.array(test.brand_name),\n",
    "  \"category\" : np.array(test.category),\n",
    "  \"item_condition\" : np.array(test.item_condition_id),\n",
    "  \"shipping\" : np.array(test[[\"shipping\"]]),\n",
    "  \"desc_len\" : np.array(test[[\"desc_len\"]]),\n",
    "  \"name_len\" : np.array(test[[\"name_len\"]]),\n",
    "  \"subcat_0\" : np.array(test.subcat_0),\n",
    "  \"subcat_1\" : np.array(test.subcat_1),\n",
    "  \"subcat_2\" : np.array(test.subcat_2),\n",
    "  \"combined_text\" : pad_sequences(test.seq_combined, maxlen= combined_padding)\n",
    "  }\n",
    "  X_test = te_data\n",
    "  json_file = open('/content/drive/My Drive/best_model_cl.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  loaded_model = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  loaded_model.load_weights(\"/content/drive/My Drive/best_model.h5\")\n",
    "  y_pred = loaded_model.predict(X_test)\n",
    "  #test_pred = np.expm1(y_pred)\n",
    "  score = rmsle(y, y_pred)\n",
    "  return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8150,
     "status": "ok",
     "timestamp": 1587819140636,
     "user": {
      "displayName": "Bunny Ryan",
      "photoUrl": "https://lh4.googleusercontent.com/-qiGfq9smB-Q/AAAAAAAAAAI/AAAAAAAAG8A/yJ_CVhzEkko/s64/photo.jpg",
      "userId": "15945980258528082766"
     },
     "user_tz": -330
    },
    "id": "x2Fv8OUyXlbW",
    "outputId": "af786e0e-e56b-4e48-80c4-e929b708d82d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actal price : 8.0 ,Predicted price: [[9.847242]]\n",
      "Rmsle Score: 0.18668628\n"
     ]
    }
   ],
   "source": [
    "# Testing  Both the functions.\n",
    "train = pd.read_table('/content/drive/My Drive/train.tsv')\n",
    "data_point = list(train.loc[10])\n",
    "price = train['price'].loc[10]\n",
    "\n",
    "# We get sample data point from Train data and see the actual price and predicted price and rmsle score.\n",
    "print(\"The actal price :\", price ,\",Predicted price:\", final_fun_1(data_point))\n",
    "print(\"Rmsle Score:\",final_fun_2(data_point,price) )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPiC4Sps7gnC0KJ8fp3tdVb",
   "collapsed_sections": [],
   "mount_file_id": "1frfrYOgUoCQAELjWLN9GysxxfAD-KcAs",
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
